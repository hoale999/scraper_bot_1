import requests
from bs4 import BeautifulSoup
import time

def fetch_vcb_news(seen_ids):
    """
    H√†m c√†o tin t·ª©c t·ª´ VCB (ƒê√£ b·ªï sung field 'date').
    """
    # ... (Ph·∫ßn import v√† setup gi·ªØ nguy√™n) ...
    from datetime import datetime # Nh·ªõ import n·∫øu thi·∫øu
    current_year = str(datetime.now().year)

    # 1. C·∫•u h√¨nh danh s√°ch URL API c·∫ßn qu√©t
    api_urls = [
        "https://vietcombank.com.vn/sxa/InvestmentApi/InvestmentDetailResults/?l=vi-VN&s={3B4CF33A-7B38-431C-B2C5-42EBBE48896A}&itemid={158CFC95-E771-4FC2-B6EA-1D93BCD69E70}&sig=investment-detail&o=SortOrder,Descending&v={93B61FD8-B8A6-48CA-B2B0-1C9494F79C93}&investmentFacetSource={2B981AA6-1CC7-4C36-8A64-85D2F82E21A5}&investmentdocumentmenu=B%C3%A1o%20c%C3%A1o%20T%C3%A0i%20ch%C3%ADnh&investmentdocumentchip=B%C3%A1o%20c%C3%A1o%20%C4%91%E1%BB%8Bnh%20k%E1%BB%B3&investmentdocumentyear=N%C4%83m%202025&p=200",
        "https://vietcombank.com.vn/sxa/InvestmentApi/InvestmentDetailResults/?l=vi-VN&s={3B4CF33A-7B38-431C-B2C5-42EBBE48896A}&itemid={158CFC95-E771-4FC2-B6EA-1D93BCD69E70}&sig=investment-detail&o=SortOrder,Descending&v={93B61FD8-B8A6-48CA-B2B0-1C9494F79C93}&investmentFacetSource={2B981AA6-1CC7-4C36-8A64-85D2F82E21A5}&investmentdocumentmenu=%C4%90%E1%BA%A1i%20h%E1%BB%99i%20%C4%91%E1%BB%93ng%20c%E1%BB%95%20%C4%91%C3%B4ng%20b%E1%BA%A5t%20th%C6%B0%E1%BB%9Dng&investmentdocumentchip=%C4%90%E1%BA%A1i%20h%E1%BB%99i%20%C4%91%E1%BB%93ng%20c%E1%BB%95%20%C4%91%C3%B4ng&investmentdocumentyear=N%C4%83m%202025&p=200",
        "https://vietcombank.com.vn/sxa/InvestmentApi/InvestmentDetailResults/?l=vi-VN&s={3B4CF33A-7B38-431C-B2C5-42EBBE48896A}&itemid={158CFC95-E771-4FC2-B6EA-1D93BCD69E70}&sig=investment-detail&o=SortOrder,Descending&v={93B61FD8-B8A6-48CA-B2B0-1C9494F79C93}&investmentFacetSource={2B981AA6-1CC7-4C36-8A64-85D2F82E21A5}&investmentdocumentmenu=%C4%90%E1%BA%A1i%20h%E1%BB%99i%20%C4%91%E1%BB%93ng%20c%E1%BB%95%20%C4%91%C3%B4ng%20th%C6%B0%E1%BB%9Dng%20ni%C3%AAn%20n%C4%83m%202025&investmentdocumentchip=%C4%90%E1%BA%A1i%20h%E1%BB%99i%20%C4%91%E1%BB%93ng%20c%E1%BB%95%20%C4%91%C3%B4ng&investmentdocumentyear=N%C4%83m%202025&p=200"
    ]

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    new_items = []

    for url in api_urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200: continue

            data = response.json()
            if data.get('Count', 0) == 0: continue

            sections = data.get('SectionResults', [])
            for section in sections:
                results = section.get('Results', [])
                for item in results:
                    news_id = item.get('Id')
                    if news_id in seen_ids: continue
                    
                    raw_html = item.get('Html', '')
                    if not raw_html: continue

                    soup = BeautifulSoup(raw_html, 'html.parser')
                    a_tag = soup.find('a')
                    
                    if a_tag:
                        relative_link = a_tag.get('href')
                        title = a_tag.get_text(strip=True)
                        full_link = f"https://vietcombank.com.vn{relative_link}"
                        
                        # --- FIX: L·∫§Y NG√ÄY HO·∫∂C G√ÅN M·∫∂C ƒê·ªäNH ---
                        # VCB API c√≥ tr·∫£ v·ªÅ PublishDate, ta l·∫•y lu√¥n cho x·ªãn
                        # D·∫°ng: /Date(1713546000000)/ -> C·∫ßn x·ª≠ l√Ω h∆°i c·ª±c, n√™n ta g√°n t·∫°m current_year
                        # V√¨ URL ƒë√£ filter s·∫µn nƒÉm 2025 r·ªìi
                        
                        new_items.append({
                            "source": "Vietcombank",
                            "id": news_id,
                            "title": title,
                            "date": current_year, # <--- ƒê√É B·ªî SUNG DATE
                            "link": full_link
                        })
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[VCB] Exception: {e}")
            continue

    return new_items

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time

# --- C·∫§U H√åNH C√ÅC TRANG C·∫¶N C√ÄO ---
# B·∫°n th√™m b·ªõt link tho·∫£i m√°i ·ªü ƒë√¢y m√† kh√¥ng c·∫ßn s·ª≠a code logic b√™n d∆∞·ªõi
VIETIN_CONFIG = [
    {
        "name": "ƒêHƒêCƒê (Shareholder)",
        "url": "https://investor.vietinbank.vn/ShareholderMeetings.aspx",
        "payload_key": "Cart_ctl00_webPartManager_wp218868305_wp414346945_cbEvents_Callback_Param",
        # Selector d√πng ƒë·ªÉ t√¨m v√πng ch·ª©a tin (Shareholder d√πng p.event_title)
        "selector_tag": "p", 
        "selector_class": "event_title",
        "container_id": None # Kh√¥ng c·∫ßn l·ªçc theo ID b·∫£ng
    },
    {
        "name": "C√¥ng b·ªë th√¥ng tin (Filings)",
        "url": "https://investor.vietinbank.vn/Filings.aspx",
        "payload_key": "Cart_ctl00_webPartManager_wp1515247873_wp473486273_cbNews_Callback_Param",
        # Filings d√πng div.rpt_title
        "selector_tag": "div",
        "selector_class": "rpt_title",
        "container_id": None
    },
    {
        "name": "B√°o c√°o t√†i ch√≠nh (Reports)",
        "url": "https://investor.vietinbank.vn/Download.aspx",
        "payload_key": "Cart_ctl00_webPartManager_wp1220103785_wp1185227757_cbReportsMerge_Callback_Param",
        # BCTC ƒë·∫∑c bi·ªát h∆°n: Ph·∫£i t√¨m trong b·∫£ng c√≥ ID c·ª• th·ªÉ ƒë·ªÉ lo·∫°i b·ªè c√°i "Individual"
        "selector_tag": "tr", # T√¨m c√°c d√≤ng trong b·∫£ng
        "selector_class": None,
        "container_id": "tblReportsMerge" # <--- QUAN TR·ªåNG: Ch·ªâ c√†o trong b·∫£ng H·ª£p Nh·∫•t
    },
    {
        "name": "S·ª± ki·ªán kh√°c (Other Events)",
        "url": "https://investor.vietinbank.vn/OtherEvents.aspx",
        "payload_key": "Cart_ctl00_webPartManager_wp89254061_wp1184749212_cbEvents_Callback_Param",
        # Th∆∞·ªùng c·∫•u tr√∫c gi·ªëng Shareholder
        "selector_tag": "p",
        "selector_class": "event_title",
        "container_id": None
    }
]

def fetch_all_vietinbank(seen_ids):
    """
    H√†m t·ªïng h·ª£p c√†o t·∫•t c·∫£ c√°c m·ª•c c·ªßa VietinBank (ƒê√£ b·ªï sung field 'date').
    """
    # ... (Ph·∫ßn config gi·ªØ nguy√™n) ...
    # ƒê·ªÉ g·ªçn, m√¨nh ch·ªâ vi·∫øt l·∫°i ƒëo·∫°n x·ª≠ l√Ω cu·ªëi v√≤ng l·∫∑p
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded",
        "X-Requested-With": "XMLHttpRequest"
    }
    
    current_year = str(datetime.now().year)
    all_new_items = []

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t VietinBank (NƒÉm {current_year}) ---")

    for config in VIETIN_CONFIG:
        try:
            key = config['payload_key']
            payload = [(key, current_year), (key, "0")]
            
            response = requests.post(config['url'], headers=headers, data=payload, timeout=20)
            if response.status_code != 200: continue

            raw_content = response.text
            match = re.search(r'<!\[CDATA\[(.*?)\]\]>', raw_content, re.DOTALL)
            if not match: continue

            html_content = match.group(1)
            soup = BeautifulSoup(html_content, 'html.parser')
            
            search_scope = soup
            if config['container_id']:
                found_container = soup.find(id=config['container_id'])
                if found_container: search_scope = found_container
                else: continue

            if config['selector_class']:
                elements = search_scope.find_all(config['selector_tag'], class_=config['selector_class'])
            else:
                elements = search_scope.find_all(config['selector_tag'])

            for el in elements:
                a_tag = el.find('a')
                if not a_tag: continue
                
                link = a_tag.get('href')
                title = a_tag.get_text(strip=True)
                if not link or not title: continue
                if "javascript" in link.lower(): continue

                if not link.startswith("http"):
                    full_link = f"https://investor.vietinbank.vn{link}"
                else:
                    full_link = link

                id_match = re.search(r'(\d+)\.aspx', full_link)
                news_id = id_match.group(1) if id_match else full_link
                
                if news_id in seen_ids: continue

                all_new_items.append({
                    "source": f"VietinBank - {config['name']}",
                    "id": news_id,
                    "title": title,
                    "date": current_year, # <--- ƒê√É B·ªî SUNG DATE
                    "link": full_link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[{config['name']}] L·ªói ngo·∫°i l·ªá: {e}")
            continue

    return all_new_items

import requests
import re
import html
import ssl
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

def fetch_bidv_data(seen_ids):
    """
    H√†m c√†o t·ªïng h·ª£p BIDV:
    1. Th√¥ng tin c·ªï ƒë√¥ng
    2. B√°o c√°o v√† T√†i li·ªáu (BCTC)
    3. L·ªãch s·ª± ki·ªán
    
    Ch·ªâ l·∫•y d·ªØ li·ªáu c·ªßa nƒÉm hi·ªán t·∫°i (2025).
    """
    
    # Danh s√°ch c√°c link c·∫ßn c√†o
    target_urls = [
        "https://bidv.com.vn/vn/quan-he-nha-dau-tu/thong-tin-co-dong",
        "https://bidv.com.vn/vn/quan-he-nha-dau-tu/bao-cao-va-tai-lieu",
        "https://bidv.com.vn/vn/quan-he-nha-dau-tu/lich-su-kien"
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    current_year = datetime.now().year
    new_items = []
    
    # T·∫°o session chung
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    for url in target_urls:
        try:
            # print(f"--- ƒêang qu√©t: {url} ---")
            response = session.get(url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                print(f"[BIDV] L·ªói k·∫øt n·ªëi {url}: {response.status_code}")
                continue

            raw_content = response.text

            # --- REGEX X·ª¨ L√ù ƒêA NƒÇNG ---
            
            # Pattern 1: D√†nh cho T√ÄI LI·ªÜU (c√≥ file_title ƒë·ªÉ t·∫£i PDF)
            # C·∫•u tr√∫c: title: ..., publishdate: ..., file_title: ...
            pattern_doc = r"title:\s*formatTitle\('(.*?)'\),\s*publishdate:\s*'(.*?)',\s*file_title:\s*formatTitle\('(.*?)'\)"
            matches_doc = re.findall(pattern_doc, raw_content)

            # Pattern 2: D√†nh cho S·ª∞ KI·ªÜN / TIN T·ª®C (c√≥ path ƒë·ªÉ xem chi ti·∫øt)
            # C·∫•u tr√∫c: title: ..., publishdate: ..., ..., path: ...
            # D√πng non-greedy (.*?) ƒë·ªÉ tr√°nh nu·ªët qu√° nhi·ªÅu k√Ω t·ª±
            pattern_event = r"title:\s*formatTitle\('(.*?)'\),\s*publishdate:\s*'(.*?)'[\s\S]*?path:\s*'(.*?)'"
            matches_event = re.findall(pattern_event, raw_content)
            
            # G·ªôp 2 danh s√°ch l·∫°i ƒë·ªÉ x·ª≠ l√Ω
            # ƒê√°nh d·∫•u lo·∫°i ƒë·ªÉ d·ªÖ debug: (Title, Date, Link, Type)
            all_matches = [(m[0], m[1], m[2], 'DOC') for m in matches_doc] + \
                          [(m[0], m[1], m[2], 'EVENT') for m in matches_event]

            for item_match in all_matches:
                raw_title, date_str, relative_link, item_type = item_match
                
                # 1. L·ªçc d·ªØ li·ªáu r√°c
                if not relative_link or relative_link == "undefined" or relative_link == "":
                    continue
                
                # 2. L·ªåC NƒÇM (Logic quan tr·ªçng nh·∫•t)
                try:
                    # BIDV ng√†y th√°ng th∆∞·ªùng l√† dd/mm/yyyy
                    pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                    if pub_date.year != current_year:
                        continue # Kh√¥ng ph·∫£i nƒÉm nay th√¨ b·ªè qua
                except ValueError:
                    continue # L·ªói ng√†y th√°ng -> B·ªè qua

                # 3. L√†m s·∫°ch ti√™u ƒë·ªÅ
                title = html.unescape(raw_title)
                
                # 4. X·ª≠ l√Ω Link ho√†n ch·ªânh
                if not relative_link.startswith("http"):
                    full_link = f"https://bidv.com.vn{relative_link}"
                else:
                    full_link = relative_link

                # 5. T·∫°o ID v√† Check tr√πng
                news_id = full_link # D√πng link l√†m ID l√† an to√†n nh·∫•t

                if news_id in seen_ids:
                    continue
                
                # ƒê√°nh d·∫•u ngu·ªìn c·ª• th·ªÉ ƒë·ªÉ s·∫øp d·ªÖ nh√¨n
                source_name = "BIDV"
                if "lich-su-kien" in url:
                    source_name = "BIDV - S·ª± Ki·ªán"
                elif "bao-cao" in url:
                    source_name = "BIDV - BCTC & T√†i Li·ªáu"
                else:
                    source_name = "BIDV - C·ªï ƒê√¥ng"

                # 6. Th√™m v√†o danh s√°ch k·∫øt qu·∫£
                # L∆∞u √Ω: Check l·∫°i l·∫ßn n·ªØa ƒë·ªÉ tr√°nh tr√πng l·∫∑p gi·ªØa Pattern 1 v√† Pattern 2
                # (V√¨ ƒë√¥i khi 1 tin v·ª´a c√≥ path v·ª´a c√≥ file_title)
                is_duplicate_in_batch = False
                for existing in new_items:
                    if existing['id'] == news_id:
                        is_duplicate_in_batch = True
                        break
                
                if not is_duplicate_in_batch:
                    new_items.append({
                        "source": source_name,
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": full_link
                    })

        except Exception as e:
            print(f"[BIDV] L·ªói t·∫°i {url}: {e}")
            continue

    return new_items

import requests
from datetime import datetime
import time

def fetch_tcb_news(seen_ids):
    """
    H√†m c√†o Techcombank (Phi√™n b·∫£n V√©t C·∫°n).
    - X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p file n·∫±m ·ªü Parent (ch√≠nh item) v√† file n·∫±m ·ªü Children (documentItems).
    - Qu√©t ƒë·ªß 6 danh m·ª•c.
    - S·∫Øp x·∫øp th·ªùi gian chu·∫©n.
    """
    
    categories = [
        "tai-lieu",
        "nghi-quyet",
        "thong-cao-bao-chi-dhcd",
        "thong-bao-va-thu-moi",
        "bao-cao-tai-chinh-vas",
        "hoi-dong-quan-tri"
    ]

    base_url_template = "https://techcombank.com/graphql/execute.json/techcombank/viewDocumentList%3BcfPath%3D/content/dam/techcombank/master-data/vi/list-view-document/{}/"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://techcombank.com/"
    }

    current_year = datetime.now().year
    new_items = []

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t TCB (NƒÉm {current_year}) ---")

    for cat in categories:
        url = base_url_template.format(cat)
        
        try:
            response = requests.get(url, headers=headers, timeout=20)
            if response.status_code != 200:
                print(f"[TCB - {cat}] L·ªói: {response.status_code}")
                continue

            json_data = response.json()
            items = json_data.get("data", {}).get("listViewDocumentFragmentList", {}).get("items", [])
            
            if not items: continue

            for item in items:
                # --- 1. CHECK NG√ÄY TH√ÅNG (D√πng chung cho c·∫£ Parent v√† Child) ---
                date_str = item.get("date") 
                if not date_str: continue

                try:
                    pub_date = datetime.strptime(date_str, "%Y-%m-%d")
                    if pub_date.year != current_year:
                        continue
                except ValueError:
                    continue

                # L·∫•y ti√™u ƒë·ªÅ g·ªëc (Category Title)
                cat_title = item.get("categoryTitle", {}).get("plaintext", "")
                
                # --- 2. LOGIC V√âT C·∫†N (QUAN TR·ªåNG) ---
                
                # A. Ki·ªÉm tra CH√çNH N√ì (Parent) - ƒê√¢y l√† ph·∫ßn code c≈© b·ªã thi·∫øu
                parent_doc_path = item.get("documentPath")
                if parent_doc_path and isinstance(parent_doc_path, dict):
                    file_link = parent_doc_path.get("_publishUrl")
                    
                    if file_link:
                        # V·ªõi Parent, ti√™u ƒë·ªÅ ch√≠nh l√† cat_title
                        # Ki·ªÉm tra th√™m documentTitle n·∫øu c√≥
                        doc_title = item.get("documentTitle", {}).get("plaintext")
                        full_title = f"{cat_title} - {doc_title}" if doc_title else cat_title
                        
                        # Th√™m v√†o list (d√πng h√†m n·ªôi b·ªô ho·∫∑c append tr·ª±c ti·∫øp)
                        if file_link not in seen_ids:
                             new_items.append({
                                "source": f"TCB - {cat}",
                                "id": file_link,
                                "title": full_title.strip(" -"),
                                "date": date_str,
                                "link": file_link,
                                "raw_date": pub_date
                            })

                # B. Ki·ªÉm tra CON N√ì (Children) - Logic c≈©
                children = item.get("documentItems", [])
                for child in children:
                    child_doc_path = child.get("documentPath")
                    if not child_doc_path or not isinstance(child_doc_path, dict):
                        continue
                        
                    file_link = child_doc_path.get("_publishUrl")
                    if not file_link: continue

                    # Ti√™u ƒë·ªÅ con
                    sub_title = child.get("documentTitle", {}).get("plaintext", "")
                    if sub_title and sub_title.lower() != "t·∫£i file":
                        full_title = f"{cat_title} - {sub_title}"
                    else:
                        full_title = cat_title

                    if file_link not in seen_ids:
                        # Ki·ªÉm tra xem ƒë√£ th√™m ·ªü b∆∞·ªõc A ch∆∞a ƒë·ªÉ tr√°nh tr√πng l·∫∑p trong c√πng 1 v√≤ng l·∫∑p
                        is_exist = False
                        for x in new_items:
                            if x['id'] == file_link:
                                is_exist = True
                                break
                        
                        if not is_exist:
                            new_items.append({
                                "source": f"TCB - {cat}",
                                "id": file_link,
                                "title": full_title.strip(" -"),
                                "date": date_str,
                                "link": file_link,
                                "raw_date": pub_date
                            })

            time.sleep(0.2)

        except Exception as e:
            print(f"[TCB - {cat}] Exception: {e}")
            continue

    # S·∫Øp x·∫øp l·∫°i t·ª´ M·ªõi nh·∫•t -> C≈© nh·∫•t
    new_items.sort(key=lambda x: x['raw_date'], reverse=True)
    for item in new_items:
        del item['raw_date']

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def fetch_mch_news(seen_ids):
    # C·∫•u h√¨nh nƒÉm
    target_year_id = "411" 
    target_year_title = "2025"
    current_year = datetime.now().year
    
    categories = [
        "thong-tin-tai-chinh", 
        "cong-bo-thong-tin/thong-tin-cong-bo",
        "dai-hoi-dong-co-dong"
    ]

    base_url_template = "https://masanconsumer.com/quan-he-co-dong/{}/page/{}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MCH ---")

    for cat in categories:
        print(f"üìÇ ƒêang qu√©t m·ª•c: {cat}")
        
        # --- C√ÄI ƒê·∫∂T PHANH TAY: Ch·ªâ qu√©t t·ªëi ƒëa 3 trang ---
        for page in range(1, 2): 
            url_path = base_url_template.format(cat, page)
            full_url = f"{url_path}/?yearID={target_year_id}&yearTitle={target_year_title}"
            
            # print(f"   >> ƒêang t·∫£i trang {page}...") 

            try:
                response = requests.get(full_url, headers=headers, timeout=10, verify=False)
                
                # N·∫øu b·ªã redirect v·ªÅ trang 1 (d·∫•u hi·ªáu h·∫øt trang c·ªßa Masan) th√¨ d·ª´ng ngay
                if page > 1 and (response.url == base_url_template.format(cat, 1) or "page/1" in response.url):
                    print("      -> H·∫øt trang (Redirect loop detected).")
                    break

                if response.status_code == 404:
                    print("      -> H·∫øt trang (404).")
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m link PDF
                pdf_links = soup.select('a[href$=".pdf"], a[href$=".PDF"]')
                
                if not pdf_links:
                    # print("      -> Kh√¥ng th·∫•y PDF n√†o.")
                    if page == 1: continue # Trang 1 m√† ko c√≥ th√¨ l·∫°, nh∆∞ng c·ª© ti·∫øp t·ª•c
                    else: break # C√°c trang sau ko c√≥ th√¨ d·ª´ng

                count_added = 0
                for a_tag in pdf_links:
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True) or a_tag.get('title') or "T√†i li·ªáu PDF"

                    if not link: continue
                    
                    if not link.startswith('http'):
                        link = f"https://masanconsumer.com{link}"

                    # Check tr√πng
                    if link in seen_ids:
                        continue
                    
                    # Check xem link c√≥ ch·ª©a nƒÉm 2025 kh√¥ng (Optional - ƒë·ªÉ l·ªçc k·ªπ h∆°n)
                    # if "2025" not in link and "25" not in link:
                    #     continue 

                    new_items.append({
                        "source": f"MCH - {cat.split('/')[0]}",
                        "id": link,
                        "title": title,
                        "date": str(current_year),
                        "link": link
                    })
                    count_added += 1
                
                # print(f"      -> L·∫•y ƒë∆∞·ª£c {count_added} tin m·ªõi.")
                
                # N·∫øu trang n√†y kh√¥ng l·∫•y ƒë∆∞·ª£c tin n√†o m·ªõi -> Kh·∫£ nƒÉng l√† h·∫øt tin m·ªõi -> D·ª´ng lu√¥n cho nhanh
                # if count_added == 0 and page > 1:
                #    break

            except Exception as e:
                print(f"[MSN] L·ªói k·∫øt n·ªëi: {e}")
                break
            
    return new_items

import requests
import time
from datetime import datetime

def fetch_vpb_news(seen_ids):
    """
    H√†m c√†o d·ªØ li·ªáu t·ª´ VPBank (API JSON).
    - T·ª± ƒë·ªông gh√©p URL theo nƒÉm hi·ªán t·∫°i.
    - Qu√©t 4 danh m·ª•c.
    - V√©t c·∫°n c√°c file trong 'itemList'.
    """
    
    # 1. C·∫•u h√¨nh danh m·ª•c (Params)
    categories = [
        "cong-bo-thong-tin-khac",
        "dai-hoi-co-dong",
        "bao-cao-tai-chinh/vas",
        "tai-lieu-cho-nha-dau-tu/bao-cao-phan-tich-ket-qua-hoat-dong"
    ]

    # Endpoint API g·ªëc
    api_url = "https://www.vpbank.com.vn/uiux-api/api/document"
    domain = "https://www.vpbank.com.vn"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.vpbank.com.vn/"
    }

    current_year = datetime.now().year
    new_items = []

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t VPBank (NƒÉm {current_year}) ---")

    for cat in categories:
        # C·∫•u tr√∫c path: /quan-he-nha-dau-tu/{category}/{year}
        category_path = f"/quan-he-nha-dau-tu/{cat}/{current_year}"
        
        # Qu√©t t·ªëi ƒëa 3 trang (th∆∞·ªùng 1 nƒÉm kh√¥ng qu√° nhi·ªÅu tin/m·ª•c)
        for page in range(1, 2):
            params = {
                "lang": "vi",
                "categoryPath": category_path,
                "pageSize": 10, # L·∫•y 10 tin m·ªói l·∫ßn
                "pageIndex": page
            }

            try:
                response = requests.get(api_url, headers=headers, params=params, timeout=15)
                
                if response.status_code != 200:
                    print(f"[VPB] L·ªói k·∫øt n·ªëi: {response.status_code}")
                    break

                json_data = response.json()
                items = json_data.get("data", [])

                # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu -> H·∫øt trang -> D·ª´ng danh m·ª•c n√†y
                if not items:
                    break
                
                for item in items:
                    # L·∫•y th√¥ng tin chung c·ªßa b√†i vi·∫øt
                    article_title = item.get("title", "")
                    publish_date = item.get("publishDate", "") # 2025-10-17T18:00...
                    
                    # X·ª≠ l√Ω ng√†y th√°ng cho ƒë·∫πp (b·ªè ph·∫ßn gi·ªù)
                    if publish_date:
                        try:
                            date_obj = datetime.fromisoformat(publish_date)
                            date_str = date_obj.strftime("%d/%m/%Y")
                        except:
                            date_str = str(current_year)
                    else:
                        date_str = str(current_year)

                    # QUAN TR·ªåNG: L·∫•y file ƒë√≠nh k√®m trong 'itemList'
                    file_list = item.get("itemList", [])
                    
                    for file_info in file_list:
                        file_url = file_info.get("url")
                        file_title = file_info.get("title") or article_title
                        
                        if not file_url:
                            continue

                        # Gh√©p domain n·∫øu thi·∫øu
                        if not file_url.startswith("http"):
                            full_link = f"{domain}{file_url}"
                        else:
                            full_link = file_url

                        # T·∫°o ID v√† Check tr√πng
                        news_id = full_link 

                        if news_id in seen_ids:
                            continue

                        # ƒê√≥ng g√≥i
                        new_items.append({
                            "source": f"VPBank - {cat.split('/')[-1]}", # L·∫•y t√™n ng·∫Øn g·ªçn
                            "id": news_id,
                            "title": file_title, # ∆Øu ti√™n t√™n file
                            "date": date_str,
                            "link": full_link
                        })
                
                # Ngh·ªâ nh·∫π
                time.sleep(0.5)

            except Exception as e:
                print(f"[VPB] L·ªói x·ª≠ l√Ω: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import re # C·∫ßn th√™m th∆∞ vi·ªán Regex ƒë·ªÉ b·∫Øt cookie
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vgi_news(seen_ids):
    """
    H√†m c√†o Viettel Global (VGI).
    - Fix l·ªói SSL.
    - Fix l·ªói Cookie Challenge (D1N).
    """
    
    current_year = datetime.now().year
    categories = [
        "dai-hoi-dong-co-dong",
        "dieu-le-tong-cong-ty",
        "bao-cao-tai-chinh",
        "tin-co-dong"
    ]
    
    base_url_template = "https://www.viettelglobal.com.vn/{}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # T·∫°o session v√† g·∫Øn Adapter
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t VGI (NƒÉm {current_year}) ---")

    for cat in categories:
        for page in range(1, 2):
            url = base_url_template.format(cat)
            params = {"year": current_year, "page": page}
            
            try:
                # L·∫ßn g·ªçi 1: C√≥ th·ªÉ b·ªã ch·∫∑n b·ªüi Cookie Challenge
                response = session.get(url, headers=headers, params=params, timeout=20, verify=False)
                
                # --- LOGIC BYPASS COOKIE (M·ªõi th√™m) ---
                if "document.cookie" in response.text:
                    # print(f"   ! Ph√°t hi·ªán t∆∞·ªùng l·ª≠a t·∫°i {cat}, ƒëang v∆∞·ª£t qua...")
                    
                    # D√πng Regex t√¨m chu·ªói: document.cookie="KEY=VALUE"
                    # Pattern t√¨m: m·ªçi k√Ω t·ª± tr·ª´ d·∫•u " v√† d·∫•u =
                    match = re.search(r'document\.cookie="([^=]+)=([^"]+)"', response.text)
                    
                    if match:
                        cookie_name = match.group(1) # D1N
                        cookie_val = match.group(2)  # Chu·ªói m√£ h√≥a
                        
                        # G√°n cookie v√†o session
                        session.cookies.set(cookie_name, cookie_val, domain=".viettelglobal.com.vn")
                        
                        # G·ªçi l·∫°i l·∫ßn 2 (L√∫c n√†y ƒë√£ c√≥ cookie trong ng∆∞·ªùi)
                        response = session.get(url, headers=headers, params=params, timeout=20, verify=False)
                    else:
                        print(f"[VGI] Kh√¥ng gi·∫£i m√£ ƒë∆∞·ª£c cookie t·∫°i {cat}")
                        continue

                # Sau khi bypass xong, x·ª≠ l√Ω nh∆∞ b√¨nh th∆∞·ªùng
                if response.status_code != 200:
                    print(f"[VGI] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                all_links = soup.find_all('a', href=True)
                
                count_in_page = 0
                for a_tag in all_links:
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True) or a_tag.get('title')

                    if not link or not title: continue
                    
                    if not link.startswith('http'):
                        link = f"https://www.viettelglobal.com.vn{link}"
                    
                    # Logic l·ªçc file/tin
                    is_valid = False
                    lower_link = link.lower()
                    if lower_link.endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx')):
                        is_valid = True
                    elif cat in lower_link:
                        is_valid = True
                    
                    if not is_valid: continue
                    if len(title) < 5 or "xem th√™m" in title.lower(): continue

                    # Check tr√πng
                    news_id = link
                    if news_id in seen_ids: continue
                    
                    # Check tr√πng n·ªôi b·ªô
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"Viettel Global - {cat}",
                        "id": news_id,
                        "title": title,
                        "date": str(current_year),
                        "link": link
                    })
                    count_in_page += 1
                
                if count_in_page == 0:
                    break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[VGI] L·ªói t·∫°i {cat}: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL (n·∫øu c√≥)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX (Gi·ªØ nguy√™n ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt n·ªëi m∆∞·ª£t) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_hpg_news(seen_ids):
    """
    H√†m c√†o d·ªØ li·ªáu t·ª´ H√≤a Ph√°t (HPG).
    - H·ªó tr·ª£ c·∫£ d·∫°ng Tin t·ª©c (Grid) v√† T√†i li·ªáu (Table).
    - L·ªçc theo nƒÉm hi·ªán t·∫°i (sort_year).
    """
    
    current_year = datetime.now().year
    
    # Danh s√°ch c√°c m·ª•c c·∫ßn c√†o
    categories = [
        "cong-bo-thong-tin",
        "bao-cao-tai-chinh",
        "dai-hoi-co-dong"
    ]

    base_url_template = "https://www.hoaphat.com.vn/quan-he-co-dong/{}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # T·∫°o session
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t HPG (NƒÉm {current_year}) ---")

    for cat in categories:
        # Qu√©t t·ªëi ƒëa 3 trang (NƒÉm hi·ªán t·∫°i th∆∞·ªùng √≠t tin)
        for page in range(1, 2):
            url = base_url_template.format(cat)
            
            # Params chu·∫©n c·ªßa HPG
            params = {
                "sort_year": current_year,
                "page": page
            }
            
            try:
                response = session.get(url, headers=headers, params=params, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[HPG] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # --- CHI·∫æN THU·∫¨T "HYBRID PARSER" (X·ª≠ l√Ω c·∫£ 2 d·∫°ng giao di·ªán) ---
                
                found_items_in_page = 0
                
                # CASE 1: D·∫°ng Tin t·ª©c (Th∆∞·ªùng l√† c√°c div c√≥ class 'item')
                news_items = soup.select('.item')
                
                # CASE 2: D·∫°ng B·∫£ng (Th∆∞·ªùng l√† tr trong table, d√πng cho BCTC)
                table_rows = soup.select('table tr')
                
                # G·ªôp chung l·∫°i ƒë·ªÉ x·ª≠ l√Ω (l·ªçc b·ªè c√°c tr ti√™u ƒë·ªÅ)
                all_elements = news_items + [tr for tr in table_rows if tr.find('a')]

                if not all_elements:
                    if page == 1: 
                        # print(f"   [HPG - {cat}] Kh√¥ng th·∫•y d·ªØ li·ªáu ·ªü trang 1.")
                        pass
                    else:
                        break # H·∫øt trang -> D·ª´ng

                for element in all_elements:
                    # T√¨m th·∫ª a (Link v√† Title)
                    a_tag = element.find('a')
                    if not a_tag: continue
                    
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True) or a_tag.get('title')
                    
                    if not link or not title: continue

                    # T√¨m ng√†y th√°ng (HPG th∆∞·ªùng ƒë·ªÉ trong class 'time' ho·∫∑c td cu·ªëi c√πng)
                    date_str = ""
                    time_tag = element.select_one('.time')
                    if time_tag:
                        date_str = time_tag.get_text(strip=True)
                    else:
                        # N·∫øu l√† d·∫°ng b·∫£ng, th·ª≠ l·∫•y c·ªôt cu·ªëi c√πng (th∆∞·ªùng l√† ng√†y)
                        tds = element.find_all('td')
                        if tds:
                            date_str = tds[-1].get_text(strip=True)

                    # Chu·∫©n h√≥a Link
                    if not link.startswith('http'):
                        link = f"https://www.hoaphat.com.vn{link}"
                    
                    # --- CHECK TR√ôNG ---
                    news_id = link
                    if news_id in seen_ids:
                        continue
                    
                    # Check tr√πng n·ªôi b·ªô
                    if any(x['id'] == news_id for x in new_items):
                        continue
                    
                    # --- CHECK NƒÇM (Double Check) ---
                    # D√π ƒë√£ d√πng param sort_year, nh∆∞ng check th√™m cho ch·∫Øc
                    is_valid_year = True
                    if date_str:
                        try:
                            # HPG format th∆∞·ªùng l√† dd/mm/yyyy
                            pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                            if pub_date.year != current_year:
                                is_valid_year = False
                        except:
                            pass # L·ªói parse ng√†y th√¨ c·ª© tin v√†o param sort_year c·ªßa server
                    
                    if not is_valid_year:
                        continue

                    new_items.append({
                        "source": f"Hoa Phat - {cat}",
                        "id": news_id,
                        "title": title,
                        "date": date_str or str(current_year),
                        "link": link
                    })
                    found_items_in_page += 1
                
                if found_items_in_page == 0:
                    break # H·∫øt tin ·ªü trang n√†y -> D·ª´ng qu√©t danh m·ª•c
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[HPG] L·ªói t·∫°i {cat}: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX (V·∫´n gi·ªØ ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt n·ªëi ·ªïn ƒë·ªãnh) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_acv_news(seen_ids):
    """
    H√†m c√†o tin t·ª©c t·ª´ ACV (Airports Corporation of Vietnam).
    - URL: https://acv.vn/tin-tuc/{category}/page/{page}
    - L·ªçc ch·∫∑t ch·∫Ω theo nƒÉm hi·ªán t·∫°i.
    """
    
    # L·∫•y nƒÉm hi·ªán t·∫°i (2025)
    current_year = datetime.now().year
    
    categories = [
        "bao-cao-tai-chinh",
        "dai-hoi-dong-co-dong",
        "thong-bao-co-dong"
    ]
    
    base_url_template = "https://acv.vn/tin-tuc/{}/page/{}"
    domain = "https://acv.vn"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # T·∫°o session v√† g·∫Øn Adapter
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t ACV (NƒÉm {current_year}) ---")

    for cat in categories:
        # Qu√©t 3 trang ƒë·∫ßu m·ªói m·ª•c
        for page in range(1, 2):
            url = base_url_template.format(cat, page)
            
            try:
                # ACV ƒë√¥i khi ph·∫£n h·ªìi ch·∫≠m, ƒë·ªÉ timeout 20s
                response = session.get(url, headers=headers, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[ACV] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m danh s√°ch tin (li.item)
                items = soup.select('li.item')
                
                if not items:
                    break # H·∫øt tin ho·∫∑c l·ªói c·∫•u tr√∫c -> D·ª´ng
                
                count_in_page = 0
                
                for item in items:
                    # --- 1. X·ª¨ L√ù NG√ÄY TH√ÅNG (Quan tr·ªçng) ---
                    # HTML m·∫´u: <div class="datetime"><span>16:17 | 30/10/2024</span></div>
                    date_tag = item.select_one('.datetime span')
                    if not date_tag:
                        continue
                        
                    date_raw = date_tag.get_text(strip=True) 
                    
                    try:
                        # C·∫Øt chu·ªói l·∫•y ph·∫ßn ng√†y: "30/10/2024"
                        date_part = date_raw.split('|')[-1].strip()
                        pub_date = datetime.strptime(date_part, "%d/%m/%Y")
                        
                        # L·ªåC NƒÇM: N·∫øu kh√¥ng ph·∫£i nƒÉm nay -> B·ªè qua
                        if pub_date.year != current_year:
                            continue
                    except:
                        continue # L·ªói format ng√†y -> B·ªè qua

                    # --- 2. L·∫§Y TI√äU ƒê·ªÄ & LINK ---
                    title_tag = item.select_one('.title a')
                    if not title_tag:
                        continue
                        
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get('href')
                    
                    if not link: continue
                        
                    # Gh√©p domain n·∫øu thi·∫øu
                    if not link.startswith('http'):
                        link = f"{domain}{link}"

                    # --- 3. CHECK TR√ôNG & L∆ØU ---
                    news_id = link
                    if news_id in seen_ids:
                        continue
                    
                    # Check tr√πng l·∫∑p trong c√πng 1 l·∫ßn ch·∫°y
                    if any(x['id'] == news_id for x in new_items):
                        continue

                    new_items.append({
                        "source": f"ACV - {cat}",
                        "id": news_id,
                        "title": title,
                        "date": date_part,
                        "link": link
                    })
                    count_in_page += 1
                
                # N·∫øu qu√©t c·∫£ trang m√† kh√¥ng th·∫•y tin n√†o c·ªßa nƒÉm nay -> D·ª´ng lu√¥n danh m·ª•c n√†y
                # (V√¨ tin ƒë∆∞·ª£c s·∫Øp x·∫øp theo th·ªùi gian, trang sau ch·∫Øc ch·∫Øn c≈© h∆°n)
                if count_in_page == 0:
                     break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[ACV] L·ªói ngo·∫°i l·ªá t·∫°i {cat}: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX (Chu·∫©n b√†i cho c√°c web doanh nghi·ªáp VN) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_fpt_news(seen_ids):
    """
    H√†m c√†o d·ªØ li·ªáu nh√† ƒë·∫ßu t∆∞ t·ª´ FPT.
    - Qu√©t 3 danh m·ª•c ch√≠nh: B√°o c√°o th∆∞·ªùng ni√™n, ƒêHƒêCƒê, CBTT.
    - L·ªçc theo nƒÉm hi·ªán t·∫°i (d·ª±a tr√™n param URL v√† check l·∫°i content).
    """
    
    current_year = datetime.now().year
    
    # C·∫•u h√¨nh danh m·ª•c v√† param ID t∆∞∆°ng ·ª©ng
    # L∆∞u √Ω: FPT d√πng param id ƒë·ªÉ filter server-side
    categories = [
        {
            "name": "B√°o c√°o th∆∞·ªùng ni√™n",
            "url": "https://fpt.com/vi/nha-dau-tu/bao-cao-thuong-nien",
            "id_param": f"monthly-year-{current_year}"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://fpt.com/vi/nha-dau-tu/dai-hoi-co-dong",
            "id_param": f"shareholders-year-{current_year}"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://fpt.com/vi/nha-dau-tu/thong-tin-cong-bo",
            "id_param": f"whats-year-{current_year}"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # T·∫°o session
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t FPT (NƒÉm {current_year}) ---")

    for cat in categories:
        # FPT th∆∞·ªùng show h·∫øt trong 1 trang n·∫øu filter theo nƒÉm, nh∆∞ng c·ª© loop nh·∫π 1-2 trang cho ch·∫Øc
        # Tuy nhi√™n, link FPT b·∫°n ƒë∆∞a l√† d·∫°ng filter param, th∆∞·ªùng kh√¥ng c√≥ paging ki·ªÉu /page/2 tr√™n URL n√†y
        # M√† n√≥ d√πng JS load more ho·∫∑c show all. 
        # V·ªõi requests, ta c·ª© g·ªçi link g·ªëc k√®m param id l√† l·∫•y ƒë∆∞·ª£c list ƒë·∫ßu ti√™n.
        
        full_url = f"{cat['url']}?id={cat['id_param']}"
        
        try:
            # print(f"   >> ƒêang t·∫£i: {cat['name']}...")
            response = session.get(full_url, headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[FPT] L·ªói k·∫øt n·ªëi {cat['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- CHI·∫æN THU·∫¨T T√åM KI·∫æM ---
            # FPT th∆∞·ªùng ƒë·ªÉ tin trong c√°c th·∫ª div c√≥ class ch·ª©a 'item'
            # Ho·∫∑c ta t√¨m t·∫•t c·∫£ th·∫ª 'a' c√≥ ch·ª©a link PDF ho·∫∑c link chi ti·∫øt
            
            # T√¨m v√πng n·ªôi dung ch√≠nh ƒë·ªÉ tr√°nh menu/footer (th∆∞·ªùng l√† main ho·∫∑c content)
            main_content = soup.select_one('main') or soup.select_one('.main-content') or soup
            
            # L·∫•y t·∫•t c·∫£ c√°c kh·ªëi tin (th∆∞·ªùng l√† .item ho·∫∑c .col-)
            # C√°ch an to√†n nh·∫•t: T√¨m t·∫•t c·∫£ th·∫ª 'a'
            all_links = main_content.find_all('a', href=True)
            
            count_in_cat = 0
            
            for a_tag in all_links:
                link = a_tag.get('href')
                title = a_tag.get_text(strip=True) or a_tag.get('title')

                # 1. L·ªçc r√°c
                if not link or not title: continue
                
                # 2. Chu·∫©n h√≥a Link
                # Link FPT hay c√≥ d·∫°ng /-/media/... (Sitecore)
                if link.startswith('/'):
                    link = f"https://fpt.com{link}"
                
                # 3. Logic L·ªçc "ƒê√∫ng c√°i c·∫ßn l·∫•y":
                is_valid = False
                
                # ∆Øu ti√™n 1: Link l√† file t√†i li·ªáu (PDF, DOC, ZIP)
                if link.lower().endswith(('.pdf', '.doc', '.docx', '.zip', '.rar')):
                    is_valid = True
                
                # ∆Øu ti√™n 2: Link chi ti·∫øt tin t·ª©c (th∆∞·ªùng ch·ª©a slug d√†i)
                # Tr√°nh link quay v·ªÅ trang ch·ªß, link menu ng·∫Øn
                elif len(link) > 40 and '/nha-dau-tu/' in link:
                    is_valid = True

                if not is_valid: continue

                # 4. Ki·ªÉm tra NƒÉm (Double check content)
                # Th·ª≠ t√¨m ng√†y th√°ng xung quanh th·∫ª a (parent, sibling)
                date_str = str(current_year) # M·∫∑c ƒë·ªãnh
                
                # T√¨m th·ª≠ class 'date' ho·∫∑c 'time' g·∫ßn ƒë√≥
                parent = a_tag.find_parent()
                if parent:
                    date_tag = parent.find(class_=lambda x: x and ('date' in x or 'time' in x))
                    if not date_tag: # Th·ª≠ t√¨m ·ªü √¥ng n·ªôi
                        grandparent = parent.find_parent()
                        if grandparent:
                            date_tag = grandparent.find(class_=lambda x: x and ('date' in x or 'time' in x))
                    
                    if date_tag:
                        raw_date = date_tag.get_text(strip=True)
                        # FPT format: 15/04/2025
                        try:
                             # C·ªë g·∫Øng parse ng√†y
                            import re
                            date_match = re.search(r'\d{2}/\d{2}/\d{4}', raw_date)
                            if date_match:
                                date_str = date_match.group(0)
                                parsed_year = datetime.strptime(date_str, "%d/%m/%Y").year
                                if parsed_year != current_year:
                                    continue # B·ªè qua tin nƒÉm c≈©
                        except:
                            pass

                # 5. Check tr√πng
                news_id = link
                if news_id in seen_ids: continue
                
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"FPT - {cat['name']}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": link
                })
                count_in_cat += 1

            time.sleep(0.5)

        except Exception as e:
            print(f"[FPT] L·ªói t·∫°i {cat['name']}: {e}")
            continue

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import re
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. B·ªò D·ªäCH NG√ÄY TI·∫æNG VI·ªÜT (N√¢ng c·∫•p) ---
def parse_vietnamese_date(date_str):
    if not date_str: return None
    
    # Chu·∫©n h√≥a: b·ªè ch·ªØ "ƒêƒÉng ng√†y:", chuy·ªÉn th∆∞·ªùng, b·ªè d·∫•u th·ª´a
    clean_str = date_str.lower().replace('ƒëƒÉng ng√†y:', '').strip()
    
    # X·ª≠ l√Ω c√°c bi·∫øn th·ªÉ unicode c·ªßa ch·ªØ "th√°ng" (n·∫øu c√≥)
    # ƒê∆°n gi·∫£n nh·∫•t l√† x√≥a ch·ªØ "th√°ng" ƒëi, ch·ªâ gi·ªØ l·∫°i s·ªë ng√†y, t√™n th√°ng, nƒÉm
    clean_str = re.sub(r'th\w+ng', '', clean_str) # X√≥a t·ª´ b·∫Øt ƒë·∫ßu b·∫±ng th...ng
    
    # B·∫£ng m√£ th√°ng (c·∫≠p nh·∫≠t th√™m c√°c bi·∫øn th·ªÉ)
    month_mapping = {
        'm·ªôt': '01', 'gi√™ng': '01', 'jan': '01',
        'hai': '02', 'feb': '02',
        'ba': '03', 'mar': '03',
        't∆∞': '04', 'b·ªën': '04', 'apr': '04',
        'nƒÉm': '05', 'may': '05',
        's√°u': '06', 'jun': '06',
        'b·∫£y': '07', 'jul': '07',
        't√°m': '08', 'aug': '08',
        'ch√≠n': '09', 'sep': '09',
        'm∆∞·ªùi m·ªôt': '11', 'nov': '11', # Check th√°ng gh√©p tr∆∞·ªõc
        'm∆∞·ªùi hai': '12', 'ch·∫°p': '12', 'dec': '12',
        'm∆∞·ªùi': '10', 'oct': '10', # Check th√°ng ƒë∆°n sau
    }
    
    # Thay th·∫ø t√™n th√°ng b·∫±ng s·ªë
    for key, val in month_mapping.items():
        # D√πng regex ƒë·ªÉ thay th·∫ø nguy√™n t·ª´ (word boundary) tr√°nh nh·∫ßm l·∫´n
        if re.search(r'\b' + key + r'\b', clean_str):
            clean_str = re.sub(r'\b' + key + r'\b', val, clean_str)
            break
            
    try:
        # L√∫c n√†y chu·ªói s·∫Ω c√≥ d·∫°ng "03  02 2025" (nhi·ªÅu kho·∫£ng tr·∫Øng)
        # D√πng regex ƒë·ªÉ l·∫•y 3 c·ª•m s·ªë: ng√†y, th√°ng, nƒÉm
        numbers = re.findall(r'\d+', clean_str)
        if len(numbers) >= 3:
            day, month, year = numbers[0], numbers[1], numbers[2]
            return datetime(int(year), int(month), int(day))
        return None
    except:
        return None

# --- 2. C·∫§U H√åNH SSL ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_gas_news(seen_ids):
    current_year = datetime.now().year
    new_items = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t GAS (NƒÉm {current_year}) ---")

    # --- LINK 1: TIN C√îNG B·ªê (pgrid/574) ---
    print("   >> [1/2] Qu√©t Tin c√¥ng b·ªë (CBTT)...")
    # URL c√≥ ch·ª©a % m√£ h√≥a
    base_url_1 = "https://www.pvgas.com.vn/quan-he-co-%C4%91ong/pgrid/574/pageid/{}"
    
    for page in range(1, 2):
        url = base_url_1.format(page)
        try:
            response = session.get(url, headers=headers, timeout=20, verify=False)
            if len(response.text) < 500: break 

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # C·∫≠p nh·∫≠t Selector theo file gas_2.devtools
            # T√¨m c√°c kh·ªëi tin trong class EDN_article
            articles = soup.select('.EDN_article')
            
            found_in_page = 0
            for art in articles:
                # 1. Ng√†y th√°ng: span.EDN_simpleDate
                date_tag = art.select_one('.EDN_simpleDate')
                if not date_tag: continue
                
                pub_date = parse_vietnamese_date(date_tag.get_text(strip=True))
                if not pub_date or pub_date.year != current_year: continue

                # 2. Link & Title: h3.simpleArticleTitle a
                title_tag = art.select_one('.simpleArticleTitle a')
                if not title_tag: continue
                
                title = title_tag.get('title') or title_tag.get_text(strip=True)
                link = title_tag.get('href')
                
                if not link: continue
                if not link.startswith('http'):
                    link = f"https://www.pvgas.com.vn{link}"

                # 3. Check tr√πng & L∆∞u
                if link in seen_ids: continue
                
                new_items.append({
                    "source": "GAS - CBTT",
                    "id": link,
                    "title": title,
                    "date": pub_date.strftime("%d/%m/%Y"),
                    "link": link
                })
                found_in_page += 1
            
            if found_in_page == 0: break

        except Exception as e:
            print(f"[GAS-P1] L·ªói: {e}")

    # --- LINK 2: T√ÄI LI·ªÜU C·ªî ƒê√îNG (Ch·ªâ l·∫•y BCTC) ---
    print("   >> [2/2] Qu√©t T√†i li·ªáu c·ªï ƒë√¥ng (Ch·ªâ l·∫•y BCTC)...")
    url_2 = "https://www.pvgas.com.vn/quan-he-co-%C4%91ong/tai-lieu-co-%C4%91ong"
    
    try:
        response = session.get(url_2, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Qu√©t r·ªông: T√¨m t·∫•t c·∫£ th·∫ª article, tr, div
        all_elements = soup.find_all(['article', 'tr', 'div'])
        
        keywords = ["b√°o c√°o t√†i ch√≠nh", "bctc", "financial report"]
        
        for el in all_elements:
            # 1. L·ªçc theo NƒÉm
            has_year = False
            # T√¨m th·∫ª time ho·∫∑c t√¨m text nƒÉm tr·ª±c ti·∫øp
            if el.find('time'):
                d_str = el.find('time').get_text(strip=True)
                d = parse_vietnamese_date(d_str)
                if d and d.year == current_year: has_year = True
            elif str(current_year) in el.get_text():
                has_year = True
            
            if not has_year: continue

            # 2. T√¨m Link
            a_tag = el.find('a', href=True)
            if not a_tag: continue
            
            link = a_tag.get('href')
            title = a_tag.get_text(strip=True)
            
            # 3. L·ªçc t·ª´ kh√≥a BCTC
            is_bctc = False
            for kw in keywords:
                if kw in title.lower():
                    is_bctc = True
                    break
            if not is_bctc: continue

            if not link.startswith('http'):
                link = f"https://www.pvgas.com.vn{link}"

            if link in seen_ids: continue
            if any(x['id'] == link for x in new_items): continue

            new_items.append({
                "source": "GAS - BCTC",
                "id": link,
                "title": title,
                "date": str(current_year),
                "link": link
            })

    except Exception as e:
        print(f"[GAS-P2] L·ªói: {e}")

    return new_items

import requests
import json
from datetime import datetime
import time
import urllib3
import ssl
import re # C·∫ßn regex ƒë·ªÉ b√≥c t√°ch link t·ª´ chu·ªói HTML
from bs4 import BeautifulSoup # D√πng BS4 ƒë·ªÉ x·ª≠ l√Ω ƒëo·∫°n HTML trong JSON cho an to√†n
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_lpb_news(seen_ids):
    """
    H√†m c√†o LPBank (ƒê√£ fix theo file page.txt).
    - ƒê∆∞·ªùng d·∫´n JSON: data -> content.
    - Ng√†y th√°ng: ISO 8601 string.
    - Link: Parse t·ª´ tr∆∞·ªùng HTML 'content'.
    """
    
    current_year = datetime.now().year
    
    categories = [
        "CONG_BO_THONG_TIN",
        "BAO_CAO.BAO_CAO_TAI_CHINH", 
        "DAI_HOI_CO_DONG"
    ]

    api_url = "https://lpbank.com.vn/api/content-service/public/findAllInvestor"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/json",
        "Referer": "https://lpbank.com.vn/"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t LPBank (NƒÉm {current_year}) ---")

    for cat in categories:
        for page in range(1): 
            payload = {
                "title": None,
                "category": cat,
                "subCategory": None,
                "year": str(current_year),
                "otherYear": None,
                "page": page,
                "size": 20,
                "sortCustoms": [{"sortAsc": False, "nullsFirst": False, "sortField": "updatedDate"}]
            }

            try:
                response = session.post(api_url, headers=headers, json=payload, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[LPB] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                    break

                json_resp = response.json()
                
                # 1. L·∫•y list tin (data -> content)
                data_block = json_resp.get("data")
                if not data_block: break
                
                items = data_block.get("content", [])
                if not items:
                    if page == 0: pass
                    break

                count_in_page = 0
                for item in items:
                    title = item.get("title")
                    if not title: continue

                    # 2. X·ª≠ l√Ω ng√†y th√°ng (ISO String)
                    # VD: '2025-12-03T09:52:27.225+00:00'
                    date_raw = item.get("startDate") or item.get("createdDate")
                    date_str = str(current_year)
                    
                    if date_raw:
                        try:
                            # C·∫Øt chu·ªói l·∫•y ph·∫ßn YYYY-MM-DD (10 k√Ω t·ª± ƒë·∫ßu)
                            # C√°ch n√†y nhanh v√† an to√†n h∆°n parse full ISO
                            date_part = date_raw[:10] 
                            pub_date = datetime.strptime(date_part, "%Y-%m-%d")
                            
                            if pub_date.year != current_year:
                                continue
                            
                            date_str = pub_date.strftime("%d/%m/%Y")
                        except:
                            pass

                    # 3. L·∫•y Link t·ª´ tr∆∞·ªùng 'content' (HTML)
                    # N·ªôi dung file.txt cho th·∫•y link n·∫±m trong th·∫ª <a href="..."> b√™n trong tr∆∞·ªùng 'content'
                    html_content = item.get("content", "")
                    link = None
                    
                    if html_content:
                        # D√πng Regex ho·∫∑c BeautifulSoup ƒë·ªÉ moi link ra
                        # Regex t√¨m href="..."
                        match = re.search(r'href="([^"]+)"', html_content)
                        if match:
                            link = match.group(1)
                        else:
                            # N·∫øu kh√¥ng c√≥ link trong content, th·ª≠ d√πng slug
                             slug = item.get("slug")
                             if slug: link = f"https://lpbank.com.vn/nha-dau-tu/{slug}"

                    if not link: continue

                    # 4. Check tr√πng & L∆∞u
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"LPBank - {cat}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                if count_in_page == 0: break
                time.sleep(0.5)

            except Exception as e:
                print(f"[LPB] L·ªói t·∫°i {cat}: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vnm_news(seen_ids):
    """
    H√†m c√†o Vinamilk (VNM) - Phi√™n b·∫£n V√©t C·∫°n.
    - Link 1: B√°o c√°o t√†i ch√≠nh (/financial)
    - Link 2: B√°o c√°o th∆∞·ªùng ni√™n (/annual)
    - Link 3: ƒêHƒêCƒê (/amg) - M·ªõi th√™m
    """
    
    current_year = datetime.now().year
    
    # Danh s√°ch URL c·∫ßn qu√©t (ƒë√£ g·∫Øn param l·ªçc nƒÉm)
    target_urls = [
        f"https://www.vinamilk.com.vn/investor/reports/financial?year={current_year}",
        f"https://www.vinamilk.com.vn/investor/reports/amg?year={current_year}"
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.vinamilk.com.vn/"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t Vinamilk (NƒÉm {current_year}) ---")

    for url in target_urls:
        # X√°c ƒë·ªãnh t√™n ngu·ªìn d·ª±a tr√™n URL ƒë·ªÉ d·ªÖ theo d√µi
        source_type = "Kh√°c"
        if "financial" in url: source_type = "BCTC"
        elif "amg" in url: source_type = "ƒêHƒêCƒê"

        try:
            # print(f"   >> ƒêang qu√©t: {source_type}...")
            response = session.get(url, headers=headers, timeout=30, verify=False)
            
            if response.status_code != 200:
                print(f"[VNM] L·ªói k·∫øt n·ªëi {source_type}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- CHI·∫æN THU·∫¨T V√âT C·∫†N ---
            # T√¨m t·∫•t c·∫£ th·∫ª <a>
            all_links = soup.find_all('a', href=True)
            
            count_in_page = 0
            for a_tag in all_links:
                link = a_tag.get('href')
                title = a_tag.get_text(strip=True) or a_tag.get('title')

                # 1. L·ªçc r√°c
                if not link or not title: continue
                if len(title) < 5: continue # Ti√™u ƒë·ªÅ qu√° ng·∫Øn -> b·ªè qua

                # 2. Chu·∫©n h√≥a Link
                if not link.startswith('http'):
                    link = f"https://www.vinamilk.com.vn{link}"

                # 3. Logic L·ªçc File T√†i Li·ªáu
                is_file = False
                lower_link = link.lower()
                
                # Case A: ƒêu√¥i file ph·ªï bi·∫øn
                if lower_link.endswith(('.pdf', '.doc', '.docx', '.zip', '.rar', '.xls', '.xlsx')):
                    is_file = True
                # Case B: Link ch·ª©a t·ª´ kh√≥a download/uploads (ƒë·∫∑c tr∆∞ng Vinamilk)
                elif 'download' in lower_link or 'uploads' in lower_link:
                    is_file = True
                
                if not is_file: continue

                # 4. Check tr√πng
                news_id = link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                # L∆∞u k·∫øt qu·∫£
                new_items.append({
                    "source": f"Vinamilk - {source_type}",
                    "id": news_id,
                    "title": title,
                    "date": str(current_year), # G√°n nƒÉm hi·ªán t·∫°i v√¨ URL ƒë√£ l·ªçc
                    "link": link
                })
                count_in_page += 1
            
            time.sleep(1)

        except Exception as e:
            print(f"[VNM] L·ªói x·ª≠ l√Ω {source_type}: {e}")
            continue

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import re
import html
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- 1. C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vjc_news(seen_ids):
    """
    H√†m c√†o Vietjet Air (VJC).
    - T·ª± ƒë·ªông gi·∫£i m√£ HTML Entities (l·ªói ph√¥ng ch·ªØ).
    - Tr√≠ch xu·∫•t ng√†y th√°ng t·ª´ t√™n file (20250417...).
    - L·ªçc nƒÉm hi·ªán t·∫°i.
    """
    
    current_year = datetime.now().year
    
    # Danh s√°ch danh m·ª•c
    categories = [
        "bao-cao-tai-chinh-quy",
        "bao-cao-tai-chinh-kiem-toan",
        "thong-tin-dinh-ky",
        "thong-tin-khac",
        "dai-hoi-dong-co-dong"
    ]
    
    base_url_template = "https://ir.vietjetair.com/Home/Menu/{}"
    domain = "https://ir.vietjetair.com"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t VJC (NƒÉm {current_year}) ---")

    for cat in categories:
        url = base_url_template.format(cat)
        
        try:
            # VJC th∆∞·ªùng load t·∫•t c·∫£ trong 1 trang, kh√¥ng ph√¢n trang r√µ r√†ng ·ªü URL
            # N√™n ta ch·ªâ c·∫ßn request 1 l·∫ßn cho m·ªói danh m·ª•c
            response = session.get(url, headers=headers, timeout=30, verify=False)
            
            if response.status_code != 200:
                print(f"[VJC] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ th·∫ª a c√≥ href
            all_links = soup.find_all('a', href=True)
            
            count_in_cat = 0
            for a_tag in all_links:
                raw_link = a_tag.get('href')
                # Gi·∫£i m√£ ti√™u ƒë·ªÅ (Fix l·ªói font ti·∫øng Vi·ªát)
                raw_title = a_tag.get_text(strip=True) or a_tag.get('title')
                title = html.unescape(raw_title) if raw_title else "T√†i li·ªáu kh√¥ng ti√™u ƒë·ªÅ"

                # 1. L·ªçc r√°c & Chu·∫©n h√≥a Link
                if not raw_link or len(raw_link) < 5: continue
                
                if not raw_link.startswith('http'):
                    link = f"{domain}{raw_link}"
                else:
                    link = raw_link
                
                # 2. Logic L·ªçc File & NƒÉm
                is_valid = False
                date_str = ""
                
                # Ki·ªÉm tra xem link c√≥ ph·∫£i file t√†i li·ªáu kh√¥ng
                lower_link = link.lower()
                if lower_link.endswith(('.pdf', '.doc', '.docx', '.zip', '.rar')):
                    
                    # --- TR√çCH XU·∫§T NG√ÄY T·ª™ LINK ---
                    # VJC hay ƒë·∫∑t t√™n file ki·ªÉu: 20250417 - VJC...
                    # Regex t√¨m chu·ªói 8 s·ªë li·ªÅn nhau (YYYYMMDD)
                    date_match = re.search(r'(\d{4})(\d{2})(\d{2})', link)
                    
                    if date_match:
                        y, m, d = date_match.groups()
                        if int(y) == current_year:
                            is_valid = True
                            date_str = f"{d}/{m}/{y}"
                    
                    # N·∫øu kh√¥ng c√≥ ng√†y trong t√™n file, ki·ªÉm tra trong ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c
                    # V√≠ d·ª•: .../nam 2025/...
                    elif str(current_year) in link:
                        is_valid = True
                        date_str = str(current_year)
                    
                    # N·∫øu kh√¥ng t√¨m th·∫•y nƒÉm trong link, th·ª≠ t√¨m trong ti√™u ƒë·ªÅ
                    elif str(current_year) in title:
                        is_valid = True
                        date_str = str(current_year)

                if not is_valid: continue

                # 3. Check tr√πng
                news_id = link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"VJC - {cat}",
                    "id": news_id,
                    "title": title, # Ti√™u ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c fix l·ªói font
                    "date": date_str,
                    "link": link
                })
                count_in_cat += 1

            # print(f"   -> T√¨m th·∫•y {count_in_cat} tin t·∫°i {cat}")
            time.sleep(1)

        except Exception as e:
            print(f"[VJC] L·ªói t·∫°i {cat}: {e}")
            continue

    return new_items

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
from datetime import datetime

def fetch_hdb_news(seen_ids):
    current_year = str(datetime.now().year)
    
    target_urls = [
        "https://hdbank.com.vn/vi/investor/thong-tin-nha-dau-tu/dai-hoi-dong-co-dong",
        "https://hdbank.com.vn/vi/investor/thong-tin-nha-dau-tu/quan-he-co-dong/cong-bo-thong-tin-thong-tin-khac",
        "https://hdbank.com.vn/vi/investor/thong-tin-nha-dau-tu/bao-cao-tai-chinh"
    ]

    new_items = []

    # --- T·ªêI ∆ØU C·∫§U H√åNH ---
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # Block ·∫£nh v√† CSS ƒë·ªÉ t·∫£i si√™u nhanh
    prefs = {
        "profile.managed_default_content_settings.images": 2, 
        "profile.managed_default_content_settings.stylesheets": 2
    }
    chrome_options.add_experimental_option("prefs", prefs)
    
    # Quan tr·ªçng: Kh√¥ng ch·ªù t·∫£i full trang, ch·ªâ c·∫ßn HTML v·ªÅ l√† ch·∫°y
    chrome_options.page_load_strategy = 'eager'

    print(f"--- üöÄ Qu√©t HDBank (Turbo Mode) ---")
    
    # Kh·ªüi t·∫°o driver 1 l·∫ßn duy nh·∫•t
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(15) # Gi·ªõi h·∫°n max 15s/trang

    try:
        for url in target_urls:
            cat_name = url.split('/')[-1]
            try:
                driver.get(url)
                
                # --- B·ªé QUA SCROLL LOOP ---
                # V√¨ d·ªØ li·ªáu ·∫©n ƒë√£ c√≥ trong HTML, ta ch·ªâ c·∫ßn ch·ªù nh·∫π 1s ƒë·ªÉ JS render c∆° b·∫£n
                # Thay v√¨ cu·ªôn 3 l·∫ßn m·∫•t 6s
                time.sleep(1.5) 
                
                # L·∫•y source ngay l·∫≠p t·ª©c
                html_content = driver.page_source
                soup = BeautifulSoup(html_content, 'html.parser')

                # --- QU√âT LINK (Logic c≈© v·∫´n ngon) ---
                all_links = soup.find_all('a', href=True)
                
                count_page = 0
                for a_tag in all_links:
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True) or a_tag.get('title')

                    if not link or len(link) < 5: continue
                    
                    # Chu·∫©n h√≥a link
                    if not link.startswith('http'):
                        link = f"https://hdbank.com.vn{link}"

                    # 1. L·ªçc File/Chi ti·∫øt
                    lower_link = link.lower()
                    is_valid_type = lower_link.endswith(('.pdf', '.doc', '.docx', '.zip')) or '/chi-tiet/' in lower_link
                    if not is_valid_type: continue

                    # 2. L·ªçc Ti√™u ƒë·ªÅ r√°c
                    if not title or len(title) < 10: 
                        # Th·ª≠ l·∫•y text t·ª´ cha (v√¨ HDB hay ƒë·ªÉ text ·ªü th·∫ª p/div bao quanh a)
                        parent = a_tag.find_parent()
                        if parent: title = parent.get_text(strip=True)[:200]
                        else: continue

                    # 3. L·ªçc NƒÇM (2025) - Qu√©t c·∫£ cha l·∫´n con
                    # N·∫øu t√¨m th·∫•y "2025" ·ªü b·∫•t c·ª© ƒë√¢u xung quanh link -> L·∫•y
                    has_year = False
                    if current_year in title or current_year in link:
                        has_year = True
                    else:
                        # Check th·∫ª cha (div ch·ª©a link)
                        parent = a_tag.find_parent()
                        if parent and current_year in parent.get_text(): has_year = True
                        # Check √¥ng n·ªôi (row ch·ª©a link)
                        elif parent and parent.parent and current_year in parent.parent.get_text(): has_year = True
                    
                    if not has_year: continue

                    # 4. Check tr√πng
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"HDBank - {cat_name}",
                        "id": link,
                        "title": title,
                        "date": current_year,
                        "link": link
                    })
                    count_page += 1
                
                # print(f"   > {cat_name}: {count_page} tin.")

            except Exception as e:
                print(f"[HDB] L·ªói load {cat_name}: {e}")
                continue

    finally:
        driver.quit()

    return new_items

import requests
import json
from datetime import datetime
import time
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
import ssl

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_acb_news(seen_ids):
    """
    H√†m c√†o d·ªØ li·ªáu t·ª´ ACB.
    - C·∫•u tr√∫c API ƒë·ªìng nh·∫•t.
    - T·ª± ƒë·ªông gh√©p Params cho t·ª´ng lo·∫°i tin.
    """
    
    current_year = datetime.now().year
    
    # ID c·ªßa Tag NƒÉm (B·∫°n cung c·∫•p l√† 1551 cho nƒÉm 2025)
    # N·∫øu sang nƒÉm 2026, c·∫ßn c·∫≠p nh·∫≠t s·ªë n√†y ho·∫∑c vi·∫øt h√†m t√¨m ID ƒë·ªông
    YEAR_TAG_ID = 1551 
    
    # C·∫•u h√¨nh c√°c danh m·ª•c c·∫ßn qu√©t
    config_categories = [
        # Nh√≥m 1: L·∫•y √Ω ki·∫øn Cƒê (G·ªôp 3 ID)
        {"name": "L·∫•y √Ω ki·∫øn Cƒê", "cat_ids": [1597, 1598, 1599], "use_year_tag": True},
        
        # Nh√≥m 2: ƒê·∫°i h·ªôi ƒêCƒê (G·ªôp 3 ID)
        {"name": "ƒê·∫°i h·ªôi ƒêCƒê", "cat_ids": [1365, 1366, 1380], "use_year_tag": True},
        
        # Nh√≥m 3: C√¥ng b·ªë th√¥ng tin
        {"name": "CBTT", "cat_ids": [656], "use_year_tag": True},
        
        # Nh√≥m 4: B√°o c√°o t√†i ch√≠nh (Kh√¥ng d√πng tag nƒÉm theo link m·∫´u)
        {"name": "BCTC", "cat_ids": [1541], "use_year_tag": False} 
    ]

    base_api = "https://acb.com.vn/api/front/v1/posts"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://acb.com.vn/"
    }

    new_items = []
    session = requests.Session()
    # session.mount('https://', LegacySSLAdapter()) # ACB th∆∞·ªùng SSL chu·∫©n, n·∫øu l·ªói th√¨ m·ªü l·∫°i d√≤ng n√†y

    print(f"--- B·∫Øt ƒë·∫ßu qu√©t ACB (NƒÉm {current_year}) ---")

    for group in config_categories:
        # Duy·ªát qua t·ª´ng Category ID trong nh√≥m
        for cat_id in group["cat_ids"]:
            # T·∫°o params c∆° b·∫£n
            params = {
                "search[categories.category_id:in]": cat_id,
                "search[is_active:in]": 1,
                "page": 1,
                "limit": 10 # L·∫•y 10 tin m·ªõi nh·∫•t
            }
            
            # Th√™m tag nƒÉm n·∫øu c·∫•u h√¨nh y√™u c·∫ßu
            if group["use_year_tag"]:
                params["search[session_tags::tags:in]"] = YEAR_TAG_ID

            try:
                # print(f"   >> ƒêang t·∫£i: {group['name']} (ID {cat_id})...")
                response = session.get(base_api, headers=headers, params=params, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[ACB] L·ªói k·∫øt n·ªëi {group['name']}: {response.status_code}")
                    continue

                json_data = response.json()
                items = json_data.get("data", [])
                
                if not items: continue

                count_in_group = 0
                for item in items:
                    # 1. L·∫•y th√¥ng tin c∆° b·∫£n
                    title = item.get("title")
                    if not title: continue

                    # 2. X·ª≠ l√Ω ng√†y th√°ng (created_at: 2025-10-22T07:20:08...)
                    created_at = item.get("created_at")
                    date_str = str(current_year)
                    if created_at:
                        try:
                            # C·∫Øt chu·ªói l·∫•y yyyy-mm-dd
                            dt_obj = datetime.strptime(created_at[:10], "%Y-%m-%d")
                            
                            # N·∫øu l√† BCTC (kh√¥ng l·ªçc tag nƒÉm), ta l·ªçc th·ªß c√¥ng b·∫±ng code
                            if not group["use_year_tag"] and dt_obj.year != current_year:
                                continue
                                
                            date_str = dt_obj.strftime("%d/%m/%Y")
                        except:
                            pass

                    # 3. L·∫•y Link File (∆Øu ti√™n featured_image -> path)
                    link = None
                    featured_img = item.get("featured_image")
                    if featured_img and isinstance(featured_img, dict):
                        link = featured_img.get("path")
                    
                    # N·∫øu kh√¥ng c√≥ file, l·∫•y link b√†i vi·∫øt (slug)
                    if not link:
                        slug = item.get("slug")
                        if slug: link = f"https://acb.com.vn/nha-dau-tu/{slug}"
                    
                    if not link: continue

                    # 4. Check tr√πng & L∆∞u
                    news_id = str(item.get("id")) # D√πng ID c·ªßa b√†i vi·∫øt l√†m key check tr√πng
                    
                    if news_id in seen_ids: continue
                    # Check tr√πng link (v√¨ ƒë√¥i khi 1 file ƒë∆∞·ª£c post l·∫°i)
                    if any(x['link'] == link for x in new_items): continue

                    new_items.append({
                        "source": f"ACB - {group['name']}",
                        "id": news_id, # L∆∞u ID b√†i vi·∫øt v√†o DB
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_group += 1
                
                # print(f"      -> T√¨m th·∫•y {count_in_group} tin.")
                time.sleep(0.5)

            except Exception as e:
                print(f"[ACB] L·ªói x·ª≠ l√Ω {group['name']}: {e}")
                continue

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import html
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o b·∫£o m·∫≠t (nh√¨n cho ƒë·ª° r·ªëi m·∫Øt)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX (Phi√™n b·∫£n m·∫°nh nh·∫•t) ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        # Cho ph√©p k·∫øt n·ªëi server c≈© (Legacy)
        ctx.options |= 0x4 
        # T·∫Øt ki·ªÉm tra t√™n mi·ªÅn v√† ch·ª©ng ch·ªâ
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_mwg_news(seen_ids):
    """
    H√†m c√†o Th·∫ø Gi·ªõi Di ƒê·ªông (MWG).
    - ƒê√£ fix l·ªói SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED.
    - C·∫•u tr√∫c: HTML tƒ©nh (Server-Side Rendering).
    """
    
    current_year = datetime.now().year
    
    url = "https://mwg.vn/cong-bo-thong-tin"
    domain = "https://mwg.vn"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # T·∫°o session v√† g·∫Øn Adapter fix l·ªói
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MWG (NƒÉm {current_year}) ---")

    try:
        # verify=False ƒë·ªÉ ch·∫Øc ch·∫Øn requests kh√¥ng check l·∫°i l·∫ßn n·ªØa
        response = session.get(url, headers=headers, timeout=20, verify=False)
        
        if response.status_code != 200:
            print(f"[MWG] L·ªói k·∫øt n·ªëi: {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # T√¨m t·∫•t c·∫£ th·∫ª <a> c√≥ class l√† 'l-list__item'
        # (D·ª±a tr√™n file mwg.txt b·∫°n g·ª≠i)
        items = soup.find_all('a', class_='l-list__item')
        
        # print(f"   > T√¨m th·∫•y {len(items)} m·ª•c tr√™n trang.")
        
        count_valid = 0
        
        for item in items:
            # 1. L·∫•y ng√†y th√°ng
            date_tag = item.find('p', class_='l-list-date')
            if not date_tag: continue
            
            date_str = date_tag.get_text(strip=True) # VD: "27/04/2025"
            
            try:
                pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                if pub_date.year != current_year:
                    continue # B·ªè qua tin c≈©
            except:
                continue # L·ªói ng√†y -> B·ªè qua

            # 2. L·∫•y Link
            link = item.get('href')
            if not link: continue
            
            # Chu·∫©n h√≥a link
            if not link.startswith('http'):
                link = f"{domain}{link}"

            # 3. L·∫•y Ti√™u ƒë·ªÅ
            title_tag = item.find('p', class_='l-list-ttl')
            raw_title = title_tag.get_text(strip=True) if title_tag else ""
            
            # Gi·∫£i m√£ k√Ω t·ª± l·ªói (VD: &#x110; -> ƒê)
            title = html.unescape(raw_title) 
            
            if not title: title = "T√†i li·ªáu MWG"

            # 4. Check tr√πng
            news_id = link
            if news_id in seen_ids: continue
            if any(x['id'] == news_id for x in new_items): continue

            new_items.append({
                "source": "MWG - CBTT",
                "id": news_id,
                "title": title,
                "date": date_str,
                "link": link
            })
            count_valid += 1

        # print(f"   > L·ªçc ƒë∆∞·ª£c {count_valid} tin c·ªßa nƒÉm {current_year}.")

    except Exception as e:
        print(f"[MWG] L·ªói ngo·∫°i l·ªá: {e}")

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
import ssl

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_msn_group_news(seen_ids):
    """
    H√†m c√†o Masan Group (MSN).
    - C·∫≠p nh·∫≠t logic l·∫•y Title t·ª´ span.text v√† thu·ªôc t√≠nh download.
    """
    
    current_year = str(datetime.now().year)
    current_date_check = datetime.now().strftime("%d/%m/%Y")
    
    # Danh s√°ch ID danh m·ª•c
    sections = [
        {"id": "12", "name": "M·ª•c 12"},
        {"id": "102", "name": "M·ª•c 102"},
        {"id": "103", "name": "M·ª•c 103 (CBTT/ƒêHƒêCƒê)"},
        {"id": "104", "name": "M·ª•c 104 (BCTC)"}
    ]

    base_url = "https://www.masangroup.com/vi/investor-relations.html/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "X-Requested-With": "XMLHttpRequest"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t Masan Group (NƒÉm {current_year}) ---")

    for section in sections:
        sec_id = section['id']
        
        for page in range(1, 2):
            params = {
                "CURRENT_PAGE": page,
                "NEWS_COUNT": 20,
                "TEMPLATE_PAGE": "investor-center/template-vertical",
                "IBLOCK_ID": "62",
                "PROPERTY_CODE[]": "file_vn",
                "PARENT_SECTION": sec_id,
                "year": current_year,
                "dateCheck": current_date_check
            }
            
            try:
                response = session.get(base_url, headers=headers, params=params, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[MSN Group] L·ªói ID {sec_id}: {response.status_code}")
                    break
                
                if len(response.text) < 100: break

                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select('.block-download')
                
                if not items: break

                count_in_page = 0
                for item in items:
                    # 1. L·∫•y Link (class="link-overlay")
                    link_tag = item.select_one('a.link-overlay')
                    if not link_tag: continue
                    link = link_tag.get('href')
                    if not link: continue

                    # 2. L·∫•y Ng√†y
                    date_tag = item.select_one('.date span')
                    date_str = date_tag.get_text(strip=True) if date_tag else str(current_year)

                    # 3. L·∫§Y TI√äU ƒê·ªÄ (LOGIC M·ªöI)
                    title = "T√†i li·ªáu Masan"
                    
                    # ∆Øu ti√™n 1: L·∫•y t·ª´ span.text (nh∆∞ b·∫°n ch·ªâ)
                    text_span = item.select_one('span.text')
                    if text_span:
                        title = text_span.get_text(strip=True)
                    else:
                        # ∆Øu ti√™n 2: L·∫•y t·ª´ thu·ªôc t√≠nh download c·ªßa a.icon-download
                        download_a = item.select_one('a.icon-download')
                        if download_a and download_a.get('download'):
                            title = download_a.get('download')
                        else:
                            # ∆Øu ti√™n 3: L·∫•y t·ª´ .name span (fallback c≈©)
                            name_span = item.select_one('.name span')
                            if name_span:
                                title = name_span.get_text(strip=True)

                    # 4. Check tr√πng & L∆∞u
                    news_id = link 
                    
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"Masan Group - {section['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                if count_in_page == 0: break
                time.sleep(0.5)

            except Exception as e:
                print(f"[MSN Group] L·ªói ID {sec_id}: {e}")
                break

    return new_items

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_gvr_news(seen_ids):
    """
    H√†m c√†o GVR (Vietnam Rubber Group) - Phi√™n b·∫£n kh·ªõp HTML Elementor.
    URL g·ªëc: https://vrg.vn/quan-he-co-dong/{category}/page/{page}/
    """
    
    current_year = datetime.now().year
    
    # Danh s√°ch danh m·ª•c b·∫°n y√™u c·∫ßu
    categories = [
        "dai-hoi-dong-co-dong",
        "bao-cao-tai-chinh",
        "tin-co-dong"
    ]
    
    base_url_template = "https://vrg.vn/quan-he-co-dong/{}/page/{}/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    
    # Setup Session v·ªõi SSL Fix
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t GVR (NƒÉm {current_year}) ---")

    for cat in categories:
        # Qu√©t 3 trang ƒë·∫ßu m·ªói danh m·ª•c (th∆∞·ªùng ƒë·ªß ph·ªß h·∫øt nƒÉm hi·ªán t·∫°i)
        for page in range(1, 2):
            url = base_url_template.format(cat, page)
            
            try:
                # print(f"   >> ƒêang t·∫£i: {cat} - Trang {page}...")
                response = session.get(url, headers=headers, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[GVR] L·ªói k·∫øt n·ªëi {cat}: {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # --- PH√ÇN T√çCH HTML D·ª∞A TR√äN ·∫¢NH SCREENSHOT ---
                # M·ªói b√†i vi·∫øt n·∫±m trong 1 kh·ªëi Loop Item
                # Class ph·ªï bi·∫øn c·ªßa Elementor Loop l√† 'e-loop-item'
                items = soup.select('.e-loop-item')
                
                if not items:
                    # Fallback: N·∫øu kh√¥ng th·∫•y class e-loop-item, th·ª≠ t√¨m container chung
                    # D·ª±a v√†o ·∫£nh: T√¨m th·∫ª h3 ch·ª©a title tr∆∞·ªõc
                    items = soup.select('.elementor-widget-theme-post-title')
                
                count_in_page = 0
                
                for item in items:
                    # N·∫øu item l√† h3 (tr∆∞·ªùng h·ª£p fallback), ta c·∫ßn t√¨m cha c·ªßa n√≥ ƒë·ªÉ ki·∫øm ng√†y th√°ng
                    container = item
                    if 'e-loop-item' in item.get('class', []):
                        container = item
                    else:
                        # Leo l√™n t√¨m container chung ch·ª©a c·∫£ Title v√† Date
                        # Th∆∞·ªùng l√† 3-4 c·∫•p div
                        container = item.find_parent(class_='e-loop-item') or item.find_parent(class_='elementor-column') or item.parent.parent
                    
                    if not container: continue

                    # 1. T√åM NG√ÄY TH√ÅNG (D·ª±a tr√™n ·∫£nh: span.elementor-icon-list-text)
                    date_tag = container.select_one('.elementor-icon-list-text')
                    if not date_tag: continue
                    
                    date_str = date_tag.get_text(strip=True) # VD: 28/12/2025
                    
                    try:
                        pub_date = datetime.strptime(date_str, "%d/%m/%Y")
                        if pub_date.year != current_year:
                            continue # B·ªè qua tin nƒÉm c≈©
                    except:
                        continue # L·ªói format ng√†y -> b·ªè qua

                    # 2. T√åM LINK & TITLE (D·ª±a tr√™n ·∫£nh: h3.elementor-heading-title a)
                    # L∆∞u √Ω: T√¨m b√™n trong container
                    title_tag = container.select_one('.elementor-heading-title a')
                    if not title_tag: continue
                    
                    link = title_tag.get('href')
                    title = title_tag.get_text(strip=True)
                    
                    if not link: continue
                    
                    # 3. CHU·∫®N H√ìA LINK
                    if not link.startswith('http'):
                        link = f"https://vrg.vn{link}"
                        
                    # 4. CHECK TR√ôNG
                    news_id = link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"GVR - {cat}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                # N·∫øu trang n√†y kh√¥ng c√≥ tin n√†o c·ªßa nƒÉm nay -> D·ª´ng loop trang (v√¨ c√°c trang sau s·∫Ω c≈© h∆°n)
                if count_in_page == 0:
                    break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[GVR] L·ªói x·ª≠ l√Ω {cat}: {e}")
                break
                
    return new_items

import requests
import json
from datetime import datetime
import time
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_mbb_news(seen_ids):
    current_year = datetime.now().year
    base_domain = "https://www.mbbank.com.vn"
    new_items = []
    
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MBB (Multi-Auth Mode - NƒÉm {current_year}) ---")

    # --- B·ªò CH√åA KH√ìA 1: D√ÄNH CHO T√ÄI CH√çNH (ID 7 & 13) ---
    # Token l·∫•y t·ª´ cURL GetListFinance b·∫°n g·ª≠i (77Po...)
    finance_headers = {
        "accept": "application/json, text/plain, */*",
        "accept-language": "en-US,en;q=0.9,vi;q=0.8",
        "mb-xsrf-token-formonline": "77PoEGcVfl7NUNBq3RpRf3s0rEVZtKIhJQOF25nDRueqh6dSoEA1PLKcSTjHnoVXZSkOyIbZZHpM1zgZiX5-bdEw9ySBjnIZ71X6Fiulr1A1",
        "priority": "u=1, i",
        "referer": "https://www.mbbank.com.vn/Investor/bao-cao-tai-chinh/2025/0//0",
        "sec-ch-ua": '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"
    }
    
    finance_cookies = {
        "ASP.NET_SessionId": "oelkfrgme0kc4ngll30qlxfh",
        "LANG_CODE": "VI",
        "f5avraaaaaaaaaaaaaaaa_session_": "PBKDJABIPPFAKCIPDJFFBPDLBNHIPFEIDIPAKNDDMLBPGAFLFLILHOMCHBHLGIEPBDIDCGDGGOGPDBGHAHNAJGBAIMKAFIMDDBLPAGKBGBLMGOBBOCOFFAPMMGAFHHDG",
        "alias_current": "",
        "f5_cspm": "1234",
        "__RequestVerificationToken": "7th4Ag_M3Z9_M_M2PR1kXOJfk-nTFHCmvFKcjIUPZKaXy33YNutZcc3Y897A-E5MDdRl8v34Q25jAx65RcsYiHejhUurIiI3SxznMKm0f7E1"
    }

    # --- B·ªò CH√åA KH√ìA 2: D√ÄNH CHO C·ªî ƒê√îNG (SHAREHOLDERS) ---
    # Token l·∫•y t·ª´ cURL GetShareholders b·∫°n g·ª≠i (7uIx...)
    shareholder_headers = {
        "accept": "application/json, text/plain, */*",
        "accept-language": "en-US,en;q=0.9,vi;q=0.8",
        "mb-xsrf-token-formonline": "7uIxGtxA3E4Hg5coPOfIqwXF5YjdvY-YzGHqsKntXP6Yi8TUlXuMors-ugxxVzsHVLrCS6VBB4jM2uuxukLwg16kz3byTvU3VAuvDXMYaQk1",
        "priority": "u=1, i",
        "referer": "https://www.mbbank.com.vn/Investor/nha-dau-tu",
        "sec-ch-ua": '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"
    }
    
    shareholder_cookies = {
        "ASP.NET_SessionId": "oelkfrgme0kc4ngll30qlxfh",
        "LANG_CODE": "VI",
        "f5avraaaaaaaaaaaaaaaa_session_": "PBKDJABIPPFAKCIPDJFFBPDLBNHIPFEIDIPAKNDDMLBPGAFLFLILHOMCHBHLGIEPBDIDCGDGGOGPDBGHAHNAJGBAIMKAFIMDDBLPAGKBGBLMGOBBOCOFFAPMMGAFHHDG",
        "__RequestVerificationToken": "7th4Ag_M3Z9_M_M2PR1kXOJfk-nTFHCmvFKcjIUPZKaXy33YNutZcc3Y897A-E5MDdRl8v34Q25jAx65RcsYiHejhUurIiI3SxznMKm0f7E1",
        "alias_current": "nha-dau-tu",
        "f5avr0884827113aaaaaaaaaaaaaaaa_cspm_": "DCNGMIAFLHDFOEEHJAIJICHMEONJMELLIIFHCLPIDCCJHIDJHHOBKIKAMPCELCBAGIICNCHIBMIDJLNEJFEADLKPDJEJAHKJKILOEMFOEJLGGKGHIMPMFPKAJOKCADJP"
    }

    # Danh s√°ch c√°c request c·∫ßn th·ª±c hi·ªán, map v·ªõi b·ªô ch√¨a kh√≥a t∆∞∆°ng ·ª©ng
    targets = [
        {
            "url": "https://www.mbbank.com.vn/api/GetListFinance/7/1/2025",
            "headers": finance_headers,
            "cookies": finance_cookies,
            "name": "BCTC (ID 7)"
        },
        {
            "url": "https://www.mbbank.com.vn/api/GetListFinance/13/1/2025",
            "headers": finance_headers, # ID 13 d√πng chung ch√¨a kh√≥a T√†i ch√≠nh
            "cookies": finance_cookies,
            "name": "B√°o c√°o kh√°c (ID 13)"
        },
        {
            "url": "https://www.mbbank.com.vn/api/GetShareholders_meeting",
            "headers": shareholder_headers, # D√πng ch√¨a kh√≥a C·ªï ƒë√¥ng ri√™ng
            "cookies": shareholder_cookies,
            "name": "ƒêHƒêCƒê"
        }
    ]

    for target in targets:
        try:
            # print(f"   >> ƒêang g·ªçi: {target['name']}...")
            response = session.get(
                target["url"], 
                headers=target["headers"], 
                cookies=target["cookies"], 
                timeout=15, 
                verify=False
            )
            
            if response.status_code != 200:
                print(f"[MBB] L·ªói HTTP {response.status_code} t·∫°i {target['name']}")
                continue

            try:
                json_data = response.json()
            except json.JSONDecodeError:
                print(f"[MBB] Kh√¥ng ph·∫£i JSON t·∫°i {target['name']}")
                continue

            # --- PARSE D·ªÆ LI·ªÜU ---
            cat_info = json_data.get("data", {})
            cat_name = cat_info.get("title", target["name"])
            
            items = json_data.get("lst", [])
            if not items: continue

            count_in_cat = 0
            for item in items:
                title = item.get("title")
                file_path = item.get("file_path")
                last_save_date = item.get("last_Save_Date")
                
                if not title or not file_path: continue

                if not file_path.startswith("http"):
                    full_link = f"{base_domain}{file_path}"
                else:
                    full_link = file_path

                date_str = str(current_year)
                if last_save_date:
                    try:
                        dt_obj = datetime.strptime(last_save_date[:10], "%Y-%m-%d")
                        date_str = dt_obj.strftime("%d/%m/%Y")
                    except: pass
                
                # Check tr√πng
                news_id = str(item.get("id"))
                if not news_id or news_id == "None": news_id = full_link
                
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"MBBank - {cat_name}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": full_link
                })
                count_in_cat += 1
            
            time.sleep(0.5)

        except Exception as e:
            print(f"[MBB] Exception t·∫°i {target['name']}: {e}")
            continue

    return new_items
